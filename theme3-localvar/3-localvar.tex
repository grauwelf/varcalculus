\documentclass[12pt,a4paper]{article}
\usepackage[warn] {mathtext}
\usepackage[cp1251]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{pgfplots}
\usepackage{wrapfig}
\usepackage[top=2cm, bottom=2cm, marginparwidth=0pt, left=3cm, right=1.5cm]{geometry}
\usepackage[ruled]{algorithm2e}

\renewcommand\qedsymbol{$\blacksquare$}
\renewcommand{\labelenumi}{\arabic{enumi}) }

\begin{document}

\newtheorem*{Def}{Определение}
\newtheorem*{Th}{Теорема}
\newtheorem*{Lem}{Лемма}
\newtheorem*{Note}{Замечание}
\newtheorem*{Ex}{Пример}

\newenvironment{Sol}{\textbf{Решение.\linebreak}}

\setcounter{section}{2}
\section{Метод локальных вариаций}

	Рассмотрим следующую вариационную задачу. Пусть требуется найти функцию
	$y(x)$, заданную на интервале $\left[a, b\right]$, удовлетворяющую
	ограничениям
	\[
		\alpha(x) \leqslant y(x) \leqslant \beta(x), 
			\quad x \in \left[a,b\right],
		\eqno{(1)}
	\]
	и доставляющую экстремум (для определенности --- минимум) функционалу
	\[
		V[y(x)] = \int\limits_a^b{F(x, y(x), y'(x))}dx.
		\eqno{(2)}
	\]
	Одно или оба ограничения (1) в некоторых частях интервала могут
	отсутствовать. Если в некоторой точке $x$ отсутствует ограничение 
	$y(x) \geqslant \alpha(x)$, то полагаем $\alpha(x) = -\infty$, а
	если ограничение $y(x) \leqslant \beta(x)$, то полагаем 
	$\beta(x) = \infty$. Ограничения вида (1) включают обычные краевые 
	условия как частный случай. Действительно, если мы хотим задать 
	ограничения
	\[
		y(a) = A, \quad y(b) = B,
	\]
	то можно положить
	\[
		\alpha(a) = \beta(a) = A, \quad \alpha(b) = \beta(b) = B.
	\]

	Опишем простейший вариант метода локальных вариаций для решения этой
	задачи. Вначале разобьем отрезок $[a,b]$ на равные отрезки точками
	$x_i = a + i\Delta x$, $\Delta x = \dfrac{b-a}{N}$, $i=\overline{0,N}$.

	Введем обозначение
	\[
		W_i(y_1, y_2) = F\left(x_i + \frac{\Delta x}{2},
				\frac{y_1+y_2}{2}, 
				\frac{y_2-y_1}{\Delta x}\right){\Delta x}.
	\]
	Нетрудно заметить, что последнее выражение есть приближенное значение
	следующего интеграла
	\[
		W_i(y_1,y_2) \approx \int\limits_{x_i}^{x_{i+1}}
			F\left(x, y(x), y'(x)\right)dx,
	\]
	при условии, что $y(x_i) = y_1$, $y(x_{i+1}) = y_2$.
	
	Обозначим $y(x_i) = y_i$, $i=\overline{0,N}$. Искомую функцию $y(x)$
	мы будем аппроксимировать кусочно-линейной функцией --- ломаной, 
	соединяющей точки $(x_i, y_i)$, а функционал $V[y(x)]$ приближенно
	заменим суммой
	\[
		V[y(x)] \approx W = \sum\limits_{i=0}^{N-1}W_i(y_i, y_{i+1}).
		\eqno{(3)}
	\]
	Ограничения на $y(x)$ в точках $x_i$ запишутся теперь в виде
	\[
		\alpha(x_i) \leqslant y_i \leqslant \beta(x_i), 
			\quad i =\overline{0,N}.
		\eqno{(4)}
	\]

	Таким образом, вместо решения непрерывной вариационной задачи (1), (2)
	будем решать ее дискретный (конечномерный) аналог --- задачу (3), (4).
	Решение этой последней задачи мы будем строить последовательными 
	приближениями.

	В качестве нулевого приближения можно взять любой набор чисел, который
	удовлетворяет ограничениям (4). Обозначим их $y^0_i$, $i = \overline{0,N}$.
	При решении реальных задач следует при выборе начального приближения
	учитывать качественные, физические и прочие априорные соображения ---
	удачный выбор может значительно ускорить сходимость метода.

	Зададимся достаточно малым положительным числом $h$, которое будем называть 
	шагом варьирования. Предположим, что уже полностью построено $n$-ое 
	приближение и первые $i$ компонент $(n+1)$-го приближения, т.е. 
	$y_0^{n+1}, y_1^{n+1}, \dots, y_{i-1}^{n+1}$, $1 \leqslant i \leqslant N$.
	Все найденные величины, очевидно, должны удовлетворять ограничениям
	задачи. В качестве возможных значений $y_i^{n+1}$ рассмотрим трех
	<<кандидатов>>: $y_i^n$, $y_i^n + h$, $y_i^n - h$. Другими словами, 
	мы {\it варьируем} кривую, надеясь найти лучшее приближение. Схематически
	этот процесс изображен на рисунке ниже.
	\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
			%axis equal image=true,	
			%scale=2,
			y=2cm,
			axis equal=false,
			axis x line=bottom,
			axis y line=left,
			xlabel=$x$,ylabel=$y$,
			xmin=0, xmax=1.5,
			ymin=0, ymax=2.25,
			xtick={0.1, 0.25, 0.4, 0.55, 0.7, 0.85, 1.0, 1.15, 1.3},
			ytick=\empty,
			xticklabels={$a$, $x_1$, $\dots$, $x_{i-1}$, $x_i$, $x_{i+1}$, 
				$\dots$, $x_{N-1}$, $b$}
			%yticklabels={$\alpha$, $\beta$}
			]
			\addplot[blue, ultra thick, samples=50, domain=0.1:1.3]
					{1.5 + 0.2*(sin(360*x/3.1416) + sin(4*180*x/3.1416))};
			\addplot[blue, ultra thick, samples=50, domain=0.1:1.3]
					{0.5 - 0.2*(cos(360*x/3.1416) + cos(4*180*x/3.1416))};
			\addplot[red, ultra thick] coordinates {
				(0.1 , 1.0)
				(0.25, 1.2)
				(0.4 , 0.9)
				(0.55, 1.0)
				(0.7 , 1.3)
				(0.85, 1.0)
				(1.0 , 1.2)
				(1.15, 1.3)
				(1.3 , 1.4)
			};
			\addplot[red, dashed] coordinates {
				(0.55, 1.0)
				(0.7 , 1.6)
				(0.85, 1.0)
			};
			\addplot[red, dashed] coordinates {
				(0.55, 1.0)
				(0.7 , 1.0)
				(0.85, 1.0)
			};
			\addplot[black, dashed, mark=''] plot coordinates { (1.3, 0) (1.3, 2.2) };
			\addplot[black, dashed, mark=''] plot coordinates { (0.1, 0) (0.1, 2.2) };
			\addplot[black, dashed, mark=''] plot coordinates { (0.55, 1.0) (0.55 ,0.0) };
			\addplot[black, dashed, mark=''] plot coordinates { (0.85, 1.0) (0.85 ,0.0) };
			\addplot[black, dashed, mark=''] plot coordinates { (0.7, 1.6) (0.7 ,0.0) };
			
			\node at (5.5cm, 0.8cm) {$\alpha(x)$};
			\node at (3cm, 4.0cm) {$\beta(x)$};
			\node at (2.3cm, 2.3cm) {$y_{i-1}^{n+1}$};
			\node at (4.3cm, 1.8cm) {$y_{i+1}^n$};
		\end{axis}
	\end{tikzpicture}
	\end{center}
	Далее, следует проверить, удовлетворяют ли две последние величины
	ограничениям (4) (для $y_i^n$ ограничения выполняются по построению).
	При нарушении ограничений соответствующая величина исключается из рассмотрения.
	После этого вычислим следующие суммы:
	\[
	\begin{aligned}
		&\Phi_i = W_{i-1}(y_{i-1}^{n+1}, y_i^n) + W_i(y_i^n, y_{i+1}^n), \\
		&\Phi_i^+ = W_{i-1}(y_{i-1}^{n+1}, y_i^n+h) + W_i(y_i^n+h, y_{i+1}^n), \\
		&\Phi_i^- = W_{i-1}(y_{i-1}^{n+1}, y_i^n-h) + W_i(y_i^n-h, y_{i+1}^n).
	\end{aligned}
	\]

	Нетрудно видеть, что эти суммы являются суммами тех двух членов выражения (3),
	которые зависят от величины $y_i$. При $i=0$ суммы состоят лишь из
	второго слагаемого, при $i=N$ --- только из первого. Если величина 
	$y_i^n + h$ (или $y_i^n-h$) не удовлетворяет ограничениям (4), то
	сумму $\Phi_i^+$ (или $\Phi_i^-$) рассчитывать не нужно, их можно считать
	равными $\infty$ (если речь идет о поиске минимума функционала).

	Далее, положим
	\[
		y_i^{n+1} = \left\{\begin{aligned}
			& y_i^n \text{ при } \Phi_i \leqslant \Phi_i^+, \Phi_i \leqslant \Phi_i^-\\
			& y_i^n+h \text{ при } \Phi_i^+ < \Phi_i, \Phi_i^+ \leqslant \Phi_i^-\\
			& y_i^n-h \text { при } \Phi_i^- < \Phi_i, \Phi_i^- < \Phi_i^+
		\end{aligned}
		\right.
	\]
	После определения $y_i^{n+1}$ переходим к нахождению величины
	$y_{i+1}^{n+1}$, которое осуществляется аналогично. Продолжая этот
	процесс вплоть до нахождения $y_N^{n+1}$, мы полностью построем
	$(n+1)$-е приближение решения.  Очевидно, в ходе итераций значение
	функционала $W$ не возрастает, т.е.  $W^{n+1} \leqslant W^n$. Если на
	некоторой итерации $y_i^{n+1} = y_i^n$, $i=\overline{0,N}$ (а значит, и
	$W_i^{n+1} = W_i^n$, то дальнейшие вычисления с данным шагом
	варьирования $h$ и шагом дискретизации $\Delta x$ не имеют смысла ---
	достигнута полная сходимость. При ограниченности области значений $y_i$,
	заданной ограничениями (4), полной сходимости при фиксированных $h$ и
	$\Delta x$ обязательно можно достигнуть за конечное число шагов.
	
	После достижения полной сходимости уменьшим шаг варьирования, например,
	вдвое и начнем описанный процесс варьирования заново. При этом в качестве
	нулевого приближения берем тот набор значений, который получили после 
	полной сходимости при предыдущем значении $h$. После сходимости при новом
	значении снова уменьшаем шаг и так далее, пока не достигнем сходимости
	при достаточно малом шаге $h$ (чем меньше шаг, тем точнее найдены
	координаты $y_i$ приближенного решения).

	Затем уменьшим шаг дискретизации $\Delta x$, например, вдвое.
	Значения начального приближения для новых итераций нельзя взять
	непосредственно из предыдщуего шага, так как неизвестны значения $y_i$
	в новых промежуточных точках (точек $x_i$ стало вдвое больше). Эти 
	неизвестные значения можно найти, интерполируя последнее (лучшее)
	решение. Имея нулевое приближения, снова задаемся таким же шагом
	варьирования, как в самом начале вычислений и проведем итерации, уменьшая
	$h$ до заданой малой величины.

	После того, как и шаг варьирования $h$, и шаг дискретизации $\Delta x$ 
	станут достаточно малы, полученная при последней полной сходимости
	итераций кусочно-линейная функция будет приближенным решением задачи
	(3), (4). Таким образом, простейший вариант реализации метода локальных
	вариаций включает в себя несколько вложенных друг в друга итерационных
	вычислительных процесса. Это процесс уменьшения шага дискретизации
	$\Delta x$, процесс уменьшения шага варьирования $h$ и итерационный
	процесс вычисления $y_i$ при фиксированных $\Delta x$ и $h$. При этом,
	пока шаг дискретизации постоянен, функционал $W$ в процессе итераций не
	возрастает. Он может увеличиться при дроблении шага $\Delta x$, т.к.
	в новых точках $x_i$ значения $y_i$ вычисляются путем интерполяции и
	проверка их <<удачности>> нет производится. Однако после начала итераций
	и уменьшения шага $h$, функционал снова монотонно уменьшается.
	
	Описанный метод позволяет найти, вообще говоря, лишь локальный минимум
	функционала. При этом для сходимости всего процесса требуется, чтобы
	выполнялось определенное соотношение между шагом дискретизации и шагом
	варьирования, а именно, чтобы $h = {\Delta x}^{1+p}$, где 
	$p \geqslant 1$ зависит от формы области, описываемой ограничениями (4). 
	На практике часто используется шаг $h = O(\Delta x^2)$. 
	
	Заметим, что одним из самых полезных достоинств метода является
	легкость, с которой можно учесть так называемые ограничения, т.е.
	ограничения вида $y(t) \in G(t)$, где $G(t)$ --- заданное множество
	допустимых значений.  Достаточно легко метод обобщается на случай,
	когда функционал вариационной задачи зависит от нескольких незвестных
	кривых, от их производных и от неизвестных кривых, зависящих от
	нескольких переменных. Другим достоинством является сравнительная
	простота алгоритма и, соответственно, простота программирования и
	отладки. Из-за этого метод пользовался популярностью в эпоху
	использования вычислительной техники II и III поколений. Наконец, объем
	памяти, используемой при расчете, вероятно, минимально возможный в
	принципе. Действительно, в целом для решения задачи достаточно хранить
	в памяти лишь массив значений $y_1, y_2, \dots, y_N$ ( счетчики и
	вспомогательные пременные не в счет), которые пересчитывается <<на
	лету>>.
	
	Существенным минусом метода является сравнительно большой объем
	вычислений.  Однако при этом дробление шага варьирования $h$ не
	приводит к увеличению количества операций, равно как и не требует
	увеличени затрат памяти. Поэтому $h$ можно уменьшать до очень малых
	значений без ущерба для производительности. 

\newpage

\end{document}















