\documentclass[12pt,a4paper]{article}
\usepackage[warn] {mathtext}
\usepackage[cp1251]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{pgfplots}
\usepackage[top=2cm, bottom=2cm, marginparwidth=0pt, left=3cm, right=1.5cm]{geometry}
\usepackage[ruled]{algorithm2e}

\renewcommand\qedsymbol{$\blacksquare$}
\renewcommand{\labelenumi}{\arabic{enumi}) }

\begin{document}

\newtheorem*{Def}{Определение}
\newtheorem*{Th}{Теорема}
\newtheorem*{Lem}{Лемма}
\newtheorem*{Note}{Замечание}
\newtheorem*{Ex}{Пример}

\newenvironment{Sol}{\textbf{Решение.\linebreak}}

\setcounter{section}{1}
\section{Метод конечных разностей}
	В лабораторной работе №1 мы рассмотрели метод решения вариационных
	задач, основанный на замене исходной задачи задачей математического
	программирования. При этом не использовалось никаких дополнительных построений,
	помимо вариационной постановки.

	Данная лабораторная работа посвящена другому классу методов, основанному на
	использовании необходимого условия экстремума функционала в простейшей 
	задаче вариационного исчисления --- уравнения Эйлера. Как известно, простейшая
	задача вида
	\[
		V[y] = \int\limits_a^b{F(x, y(x), y'(x))}dx \to extr
	\]
	где экстремум ищется на множестве функций 
	$M = \left\{y(x)\,\, |\,\, y(x) \in C^2_{[a,b]},\,\, y(a)=\alpha, y(b)=\beta \right\}$,
	при определенных условиях сводится к краевой задаче для уравнения Эйлера:
	\[
		\begin{cases}
			&\dfrac{\partial F(x, y, y')}{\partial y} - 
			\dfrac{d}{dx}\dfrac{\partial F(x, y, y')}{\partial y'} = 0 , \\
			& y(a) = \alpha, \quad y(b) = \beta .
		\end{cases}
	\]
	Идея конечно-разностных методов состоит в дискретизации краевой задачи с
	заменой производных в ней соответствующими выражениями в конечных разностях.
	Ниже мы покажем, что такой подход сводит задачу к решению системы, вообще говоря,
	нелинейных алгебраический уравнений. Порядок погрешности такого приближенного
	решения определяется шагом дискретизации и выбором конечно-разностной аппроксимации
	производных. Вначале мы рассмотрим случай линейного уравнения Эйлера, для которого
	задача сильно упрощается, а затем обратимся к общему случаю.

\subsection{Метод конечных разностей в случае линейного уравнения Эйлера}

	Пусть применение необходимого условия экстремума приводит к краевой задаче вида
	\[
		\begin{cases}
			&y''(x) = p(x)y'(x) + q(x)y(x) + r(x), \quad x \in \left[a, b\right], \\
			& y(a) = \alpha, \quad y(b) = \beta .
		\end{cases}
	\]
	Заменим производные $y'(x)$ и $y''(x)$ их конечно-разностными аппроксимациями. Сначала
	разобъем весь промежуток $\left[a, b\right]$ на $N+1$ равных подынтервалов точками
	$x_i=a+ih$, $i=\overline{0,N}$, где $h=\dfrac{b-a}{N+1}$. Величина $h$ называется шагом метода
	и во многом определяет его точность.

	Если $y(x)$ --- точное решение вариационной задачи, то в узлах разбиения
	выполняется равнство
	\[
		y''(x_i) = p(x_i)y'(x_i) + q(x_i)y(x_i) + r(x_i), i = \overline{1,N}.
	\]
	Разложим решение $y(x)$ в ряд Тейлора в окрестности точки $x_i$:
	\[
		y(x) = y(x_i) + (x-x_i)y'(x_i) + \frac{\left(x-x_i\right)^2}{2}y''(x_i) + 
			 \frac{\left(x-x_i\right)^3}{6}y'''(x_i) + 
			 \frac{\left(x-x_i\right)^4}{24}y^{(4)}(\xi_i),
	\]
	где $\xi \in (x, x_i)$. Заметим, что остаточный член мы записали в форме Лагранжа.
	В точках $x_{i+1} = x_i + h$ и $x_{i-1} = x_i - h$ получаем, соответсвенно,
	\[
		y(x) = y(x_i) + hy'(x_i) + \frac{h^2}{2}y''(x_i) + \frac{h^3}{6}y'''(x_i) + 
			 \frac{h^4}{24}y^{(4)}(\xi_i^+),
	\]
	\[
		y(x) = y(x_i) - hy'(x_i) + \frac{h^2}{2}y''(x_i) - \frac{h^3}{6}y'''(x_i) + 
			 \frac{h^4}{24}y^{(4)}(\xi_i^-),
	\]
	где $\xi_i^+ \in (x_i, x_{i+1})$ и $\xi_i^- \in (x_{i-1}, x_i)$.

	Сложив два последних равенства, получим
	\[
		y(x_{i+1}) + y(x_{i-1}) = 2y(x_i) + h^2y''(x_i) + 
			\frac{h^4}{24}\left(y^{(4)}(\xi_i^+) + y^{(4)}(\xi_i^-)\right),
	\]
	откуда находим
	\[
		y''(x_i) = \frac{1}{h^2}\biggl[y(x_{i+1}) -2y(x_i) + y(x_{i-1})\biggr] - 
			\frac{h^2}{24}\biggl(y^{(4)}(\xi_i^+) + y^{(4)}(\xi_i^-)\biggr).
	\]
	Пользуясь теоремой о среднем значении, окончательно получаем аппроксимацию $y''(x)$
	\[
		y''(x_i) = \underbrace{\frac{1}{h^2}\biggl[y(x_{i+1}) -2y(x_i) + y(x_{i-1})\biggr]}_
			{\text{центральная разность 2-го порядка}} - 
			\frac{h^4}{24}y^{(4)}(\xi_i), \quad \xi_i \in (x_{i-1}, x_{i+1}).
	\]

	Аналогичным образом можно получить формулу для первой производной:
	\[
		y'(x_i) = \frac{1}{2h}\biggl[y(x_{i+1}) - y(x_{i-1})\biggr] - 
		\frac{h^2}{6}y'''(\eta_i), \quad \eta_i \in (x_{i-1}, x_{i+1}).
	\]
	Подставим выражения для $y'$ и $y''$ в уравнение :
	\begin{multline*}
	\frac{y(x_{i+1}) -2y(x_i) + y(x_{i-1})}{h^2} = 
		p(x_i)\biggl[\frac{y(x_{i+1}) - y(x_{i-1})}{2h}\biggr] + 
		q(x_i)y(x_i) + r(x_i) - \\
		- \frac{h^2}{12}\biggl[2p(x_i)y'''(\eta_i) - y^{(4)}(\xi_i)\biggr] .
	\end{multline*}
	Если в получившемся выражении отбросить последнее слагаемое, то мы получим	
	конечно-разностную аппроксимацию уравнения Эйлера в точках $x_i$. При этом 
	погрешность аппроксимации --- $O(h^2)$.
	
	Итак, задача свелась к решению системы уравнений относительно переменных
	$y_i = y(x_i)$ следующего вида:
	\[
		\begin{cases}
			&\dfrac{-y_{i+1} + 2y_i - y_{i-1}}{h^2} + p(x_i)\biggl[\dfrac{y_{i+1}-y_{i-1}}{2h}\biggr] + 
			q(x_i)y_i = - r(x_i) , \quad i = \overline{1,N}\\
			& y_0 = \alpha, \quad y_{N+1} = \beta .
		\end{cases}
	\]
	
	Приведя подобные, получаем
	\[
		- \left(1 + \frac{h}{2}p(x_i)\right)y_{i-1}
		+ \left(2 + h^2q(x_i)\right)y_i
		- \left(1 - \frac{h}{2}p(x_i)\right)y_{i+1} = -h^2r(x_i),
	\]
	или, в матричной форме, $Ay = b$, где 
	\[
		A = \begin{pmatrix}
			 2 + h^2q(x_1)         & -1 + \frac{h}{2}p(x_1) & 0                      & \dots  & 0 \\
			 -1-\dfrac{h}{2}p(x_2) &  2 + h^2q(x_2)         & -1+\dfrac{h}{2}p(x_2)  & \dots  & 0 \\
			 0                     &  \ddots                & \ddots                 & \ddots & 0 \\
			 \vdots 	       &  \ddots                & \ddots                 & \ddots & \vdots \\
			 0                     &  \dots  & 0 & -1-\dfrac{h}{2}p(x_N) & 2 + h^2q(x_N)                   
		\end{pmatrix} ,
	\]
	\[
		b = \begin{pmatrix}
			-h^2r(x_1) + \biggl(1+\dfrac{h}{2}p(x_i)\biggr)y_0 \\
			-h^2r(x_2) \\
			 \vdots    \\
			-h^2r(x_{N-1}) \\ 
			-h^2r(x_N) + \biggl(1-\dfrac{h}{2}p(x_N)\biggr)y_{N+1}
		\end{pmatrix}
	\]

	Заметим, что матрица системы имеет трехдиагональный вид. Это позволяет решить систему
	оцень быстро, используя метод прогонки. Условия существования и единственности решения
	полученной СЛАУ определяются следующей теоремой:
	\begin{Th}
		Пусть $p(x)$, $q(x)$, $r(x)$ - непрерывные на $[a, b]$ функции и,
		кроме того, $q(x) > 0, x \in [a, b]$. Тогда при $h < \dfrac{2}{L}$, где 
		$L=\max_{x \in [a, b]}|p(x)|$, трехдиагональная система $Ay=b$ имеет единственное решение.
	\end{Th}

	Сформулированная теорема не гарантирует, что точность приближенного решения будет $O(h^2)$.
	Для этого необходимо дополнительно проверить, что $y^{(4)}(x)$ непрерывна на $[a, b]$.

	Для увеличения точности можно удлинить отрезок ряда Тейлора до слагаемого порядка $h^5$. Тогда
	можно получить конечно-разностную аппроксимацию с ошибкой $O(h^4)$. Однако при этом
	в формулах для $y'(x_i)$ и $y''(x_i)$ появятся $y(x_{i-2}), y(x_{i+2})$. Возникает проблема
	с аппроксимацией производных на краях промежутка $[a, b]$, т.к. непонятно, какие значения
	должны иметь $y_{-1}$ и $y_{N+2}$. Кроме того (и это значительно хуже), система уравнений
	перестанет быть трехдиагональной. Альтернативой такому подходу может служить метод
	экстраполяции Ричардсона.

	Формальный алгоритм метода конечных разностей для случая линейного уравнения Эйлера 
	приведен на странице 4. Ниже приведен пример, который можно использовать при отладке.
	\begin{Ex}
		Рассмотрим пример использования описанного алгоритма, решив численно
		краевую задачу для уравнения Эйлера
		\[
		\begin{cases}
			& y'' = -\dfrac{2}{x}y' + \dfrac{2}{x^2}y + \dfrac{\sin{\ln{x}}}{x^2}, \quad x \in [1,2] \\
			& y(1) = 1, \quad y(2) = 2
		\end{cases}
		\]
	\end{Ex}
	\begin{Sol}
		Положим $N=9$, так что $h=0.1$. Результаты расчетов приведены в таблице ниже:
		\begin{center}
			\setlength{\tabcolsep}{12pt}
			\begin{tabular}{ccccc}
				$x_i$ & $y_i$ & $y(x_i)$ & $\left|y_i-y(x_i)\right|$
				\\[4pt]
				\hline
				$1.0$ & $1.000000$ & $1.000000$ &    \\
				$1.1$ & $1.092601$ & $1.092629$ & $2.88 \times 10^{-5}$ \\
 				$1.2$ & $1.187043$ & $1.187084$ & $4.17 \times 10^{-5}$ \\
				$1.3$ & $1.283337$ & $1.283382$ & $4.55 \times 10^{-5}$ \\
				$1.4$ & $1.381402$ & $1.381446$ & $4.39 \times 10^{-5}$ \\
				$1.5$ & $1.481120$ & $1.481159$ & $3.92 \times 10^{-5}$ \\
				$1.6$ & $1.582360$ & $1.582392$ & $3.26 \times 10^{-5}$ \\
				$1.7$ & $1.684990$ & $1.685014$ & $2.49 \times 10^{-5}$ \\
				$1.8$ & $1.788882$ & $1.788899$ & $1.68 \times 10^{-5}$ \\
				$1.9$ & $1.893921$ & $1.893930$ & $8.41 \times 10^{-6}$ \\[1pt]
				$2.0$ & $2.000000$ & $2.000000$ &    \\[1pt]
				\hline
			\end{tabular}
		\end{center}
		\qed
	\end{Sol}
	\pagebreak
	\begin{algorithm}[H]
		\DontPrintSemicolon
		\NoCaptionOfAlgo
		\SetKwInOut{Input}{ВХОД} 
		\SetKwInOut{Output}{ВЫХОД}
		\SetKwInOut{KwPrint}{ВЫВЕСТИ}

		\Input{Целое число $N \geq 2$; точки $a,b$, краевые условия $\alpha, \beta$.}
		\BlankLine
		\Output{Приближенные значения $y_i$ для $y(x)$ в точках $x_i, i=\overline{1,N}$.}
		\BlankLine
		\nlset{ШАГ 1}  %\Indp 
				$h = \dfrac{b-a}{N+1}$ \;
				$x = a + h$ \;
				$a_1 = 2 + h^2q(x)$ \;
				$b_1 = -1 + \dfrac{h}{2}p(x)$ \;
				$d_1 = -h^2r(x) + \left(1+\dfrac{h}{2}p(x)\right)\alpha$ \;
			      %\Indm
		\nlset{ШАГ 2} \For{$i=2$ \KwTo $N-1$}{
				$x = a + ih$ \;
				$a_i = 2 + h^2q(x)$ \;
				$b_i = -1 + \dfrac{h}{2}p(x)$ \;
				$c_i = -1 - \dfrac{h}{2}p(x)$ \;
				$d_i = -h^2r(x)$ \;
				}
		\nlset{ШАГ 3}  %\Indp
				$x = b-h$ \;
				$a_N = 2 + h^2q(x)$ \;
				$c_N = -1 - \dfrac{h}{2}p(x)$ \;
				$d_N = -h^2r(x) + \left(1-\dfrac{h}{2}p(x)\right)\beta$ \;
			      %\Indm
		\nlset{ШАГ 4} Начинаем решать трехдиагональную систему уравнений (шаги 4-8) \;
				%\Indp 
				$l_1 = a_1$ \;
				$u_1 = b_1/a_1$ \;
				$z_1 = d_1/l_1$ \;
				%\Indm
		\nlset{ШАГ 5} \For {$i=2$ \KwTo $N-1$}{
				$l_i = a_i - c_iu_{i-1}$ \;
				$u_i = b_i/l_i$ \;
				$z_i = (d_i - c_iz_{i-1})/l_i$ \;
				}
		\nlset{ШАГ 6} % \Indp
				$l_N = a_N - c_Nu_{N-1}$ \;
				$z_N = (d_N - c_Nz_{N-1})/l_N$ \;
			      %\Indm
		\nlset{ШАГ 7} % \Indp
				$y_0 = \alpha$ \;
				$y_{N+1} = \beta$ \;
				$y_N = z_N$ \;
			      %\Indm 
		\nlset{ШАГ 8} \For{$i=N-1$ \KwTo $1$}{
				$y_i = z_i - u_iy_{i+1}$ \;
			      }
		\nlset{ШАГ 9} \For {$i=0$ \KwTo $N+1$}{
				$x = a + ih$ \;
				\KwPrint{$\left(x, y_i\right)$}}
		\nlset{ШАГ 10} Процедура окончена.				

		\caption{\Large Метод конечных разностей. Линейный случай.}
	\end{algorithm}
	\pagebreak

	
\subsection{Метод конечных разностей для уравнения Эйлера общего вида}
	
	Попробуем применить ту же идею к краевой задаче более общего вида:
	\[
		\begin{cases}
			&y'' = f(x, y, y'), \quad x \in \left[a, b\right], \\
			& y(a) = \alpha, \quad y(b) = \beta .
		\end{cases}
	\]
	Очевидно, что в общем случае уравнение в конечных разностях уже не будет
	линейным.

	Вначале сформулируем условия, которые в дальнейшем предполагаются выполненными:
	\begin{enumerate}
		\item функции $f, f_y, f_{y'}$ непрерывны в области 
			$D = \left\{(x,y,y') | x \in [a,b], y \in \mathbb{R}, y' \in \mathbb{R} \right\}$;
		\item $f_{y'} \geq \delta > 0$ на $D$;
		\item $\exists k, L: k = \max\limits_D|f_y(x,y,y')|, L = \max\limits_D|f_{y'}(x,y,y')|$.
	\end{enumerate}
	Эти условия обеспечивают существование и единственность решения краевой задачи.

	Как и в линейном случае, разбиваем промежуток $[a, b]$ на $N+1$ равных
	подынтервалов точками $x_i = a + ih$, $h = \dfrac{b-a}{N+1}$, $i=\overline{0,N+1}$.
	Полагая, что производная $y^{(4)}(x)$ ограничена на $[a, b]$, заменяем в уравнении
	\[
		y''(x_i) = f(x_i, y(x_i), y'(x_i))
	\]
	производные $y'$ и $y''$ выражениями в конечных разницах, полученными ранее. Получаем
	\[
		\begin{cases}
			&-\dfrac{y_{i+1} - 2y_i + y_{i-1}}{h^2} + 
				f\left(x_i, y_i, \dfrac{y_{i+1}-y_{i-1}}{2h}\right) = 0 , \quad i = \overline{1,N}\\
			& y_0 = \alpha, \quad y_{N+1} = \beta .
		\end{cases}
	\]
	Включив краевые условия в систему и домножив каждое уравнение на $h^2$, получаем окончательно
	\[\begin{cases}
		       2y_1 - y_2 + h^2f\left(x_1, y_1, \dfrac{y_2-\alpha}{2h}\right) - \alpha = 0,&\\
		-y_1 + 2y_2 - y_3 + h^2f\left(x_2, y_2, \dfrac{y_3-y_1}{2h}\right)	= 0 ,&\\
											&\vdots\\
		-y_{N-2} + 2y_{N-1} - y_N + h^2f\left(x_{N-1}, y_{N-1}, \dfrac{y_N-y_{N-2}}{2h}\right) = 0 ,&\\
		-y_{N-1} + 2y_N + h^2f\left(x_N, y_N, \dfrac{\beta-y_{N-1}}{2h}\right) - \beta = 0 .&
	\end{cases}\]
	
	Эту {\bf нелинейную} систему $N$ уравнений относительно $N$ неизвестных $y_i$ можно решать
	итерационными методами. Напомним схему метода Ньютона и детализируем его применение
	в нашем конкретном случае.

	Итак, как известно, метод Ньютона (также называемый методом Ньютона-Рафсона)
	предлагает решать систему уравнений
	\[
		f(x) = 0 \quad \Longleftrightarrow \quad f_k(x_1, x_2, \dots, x_n) = 0, k = \overline{1,n}
	\]
	задавшись некоторым начальным приближением $x^0$ и вычисляя последовательно
	\[
		x^{j+1} = x^j - J^{-1}\left(x^j\right)f\left(x^j\right) ,
	\]
	где $J(x) = \left(\dfrac{\partial f_i}{\partial x_j}\right)$ --- якобиан системы уравнений.
	При условии, что $||x - J^{-1}(x)f(x)|| < 1$, можно гарантировать сходимость
	последовательности $\left\{x^j\right\}$ к решению системы. Обычно при вычислениях
	стараются избегать обращения матрицы $J(x)$. Вместо этого, полагая 
	$\Delta x^j = x^{j+1} - x^j$, записывают систему {\bf линейных} уравнений
	\[
		J\left(x^j\right)\Delta x^j = -f\left(x^j\right). 
	\]
	Решив ее и найдя $\Delta x^j$, вычисляют $x^{j+1} = \Delta x^j + x^j$. Критерием останова
	итерационной процедуры может быть малось $||\Delta x^j||$ или близость $f(x^j)$ к нулю.

	Чтобы выписать формулу итераций для рассматриваемой задачи, вычислим якобиан $J(y)$. Замечая,
	что в $i$-ом уравнении системы отличны от нуля производные лишь по переменным $y_i$, $y_{i-1}$
	и $y_{i+1}$, можем записать, что
	\[	
	J(y_1, y_2, \dots, y_N)_{ij}=
	\begin{cases}
		-1+\dfrac{h}{2}f_{y'}\left(x_i,y_i,\dfrac{y_{i+1}-y_{i-1}}{2h}\right), & i = j-1, j=\overline{2,N} \\
		 2+h^2f_y\left(x_i,y_i,\dfrac{y_{i+1}-y_{i-1}}{2h}\right) , & i = j, j = \overline{1,N} \\
		-1-\dfrac{h}{2}f_{y'}\left(x_i,y_i,\dfrac{y_{i+1}-y_{i-1}}{2h}\right) , & i = j+1,j=\overline{1,N-1} 
	\end{cases}
	\]
	Таким образом, в методе Ньютона на $j$-ой итерации необходимо решить СЛАУ
	\begin{align*}
		J\left(y_1^j, \dots, y_N^j\right) \Delta y^j = -
			\Biggl(& 2y_1^j - y_2^j + h^2f\left(x_1, y_1^j, \dfrac{y_2^j-\alpha}{2h}\right) - \alpha , \\
		& -y_1^j + 2y_2^j - y_3^j + h^2f\left(x_2, y_2^j, \dfrac{y_3^j-y_1^j}{2h}\right) ,  \\
		& \vdots \\
		& -y_{N-2}^j + 2y_{N-1}^j - y_N^j + h^2f\left(x_{N-1}, y_{N-1}^j, \dfrac{y_N^j-y_{N-2}^j}{2h}\right) \\
		& -y_{N-1}^j + 2y_N^j + h^2f\left(x_N, y_N^j, \dfrac{\beta-y_{N-1}^j}{2h}\right) - \beta
		\Biggr)^T
	\end{align*}
	и новое приближение к решению иходной краевой (а значит, и вариационной) задачи 
	вычислять по формуле
	\[
		y_i^{j+1} = y_i^j + \Delta y_i^j, \qquad i = \overline{1,N} .
	\]
	Заметим, что $J(y)$ является трехдиагональной матрицей, так что каждая итерация
	требует не слишком много времени. Можно показать, что для нелинейных задач
	описанный метод имеет порядок точности $O(h^2)$. 

	У метода Ньютона есть большой недостаток: он требует очень хорошего начального
	приближения. Априори проверить это условие нельзя, поэтому нужно аккуратно пробовать
	разные приближения, регулируя при этом шаг. Если никаких иных предположений нет,
	в качестве $\left(y_1^0, \dots, y_N^0\right)$ можно взять ординаты прямой, 
	соединяющей точки $(a, \alpha)$ и $(b, \beta)$, вычисленные в точках $x_i$.

	Запишем теперь формальный алгоритм метода конечных разностей для общего случая.

	\pagebreak
	\begin{algorithm}[H]
		\DontPrintSemicolon
		\NoCaptionOfAlgo
		\SetKwInOut{Input}{ВХОД} 
		\SetKwInOut{Output}{ВЫХОД}
		\SetKwInOut{KwPrint}{ВЫВЕСТИ}

		\Input{Целое число $N \geq 2$; точки $a,b$, краевые условия $\alpha, \beta$,
			точность $\varepsilon$, максимальное число итераций $M$.}
		\BlankLine
		\Output{Приближенные значения $y_i$ для $y(x)$ в точках $x_i, i=\overline{1,N}$, 
			или сообщение о превышении максимального числа итераций.}
		\BlankLine
		\nlset{ШАГ 1}  %\Indp 
				$h = \dfrac{b-a}{N+1} ,\quad y_0 = \alpha ,\quad y_{N+1} = \beta$ \;
			      %\Indm
		\nlset{ШАГ 2} \For{$i = 1$ \KwTo $N$}{
				$y_i = \alpha + \dfrac{\beta-\alpha}{b-a}ih$ \;
				}
		\nlset{ШАГ 3}  %\Indp
				$j = 1$ \;
				Повторяем шаги 4-9 \;
				\While{$j \leq M$}{
		\nlset{ШАГ 4} 	$x = a + h , \quad t = \dfrac{y_2-\alpha}{2h}$ \;
				$a_1 = 2 + h^2f_y(x, y_1, t)$ \;
				$b_1 = -1 + \dfrac{h}{2}f_{y'}(x,y_1,t)$ \;
				$d_1 = -1 - (2y_1 - y_2 - \alpha + h^2f(x, y_1, t))$ \;
		\nlset{ШАГ 5}	\For{$i = 2$ \KwTo $N-1$}{
				$x = a + ih , \quad t = \dfrac{y_{i+1}-y_{i-1}}{2h}$ \;
				$a_i = 2 + h^2f_y(x, y_i, t)$ \;
				$b_i = -1 + \dfrac{h}{2}f_{y'}(x,y_i,t)$ \;
				$c_i = -1 - \dfrac{h}{2}f_{y'}(x,y_i,t)$ \;
				$d_i = -(2y_i - y_{i+1} - y_{i-1} + h^2f(x, y_i, t))$ \;
				}
		\nlset{ШАГ 6}   $x = b - h , \quad t = \dfrac{\beta-y_{N-1}}{2h}$ \;
				$a_N = 2 + h^2f_y(x, y_N, t)$ \;
				$c_N = -1 - \dfrac{h}{2}f_{y'}(x,y_N,t)$ \;
				$d_N = -(2y_N - y_{N-1} - \beta + h^2f(x, y_N, t))$ \;
		\nlset{ШАГ 7}	Решаем трехдиагональную систему уравнений. Вычисления полностью идентичны
				шагам 4-8 из предыдущего алгоритма. На выходе имеем вектор приращений $v$
				и новое приближение решения $y_i$, $i=\overline{0,N+1}$. \;
		\nlset{ШАГ 8}	\If{$||v|| \leq \varepsilon$}{
				\For{$i=0$ \KwTo $N+1$}{
					$x = a + ih$ \;
					\KwPrint{$(x, y_i)$}
				}
				Процедура окончена.
				}
		\nlset{ШАГ 9}  $j = j + 1$ \;
				}
		\nlset{ШАГ 10} \KwPrint{ Достигнут предел числа итераций. Процедура завершилась неудачно.}

		\caption{\Large Метод конечных разностей. Нелинейный случай.}
	\end{algorithm}

\end{document}















